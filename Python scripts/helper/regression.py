import inspect
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
import concurrent.futures as futures
from pycallgraphix.wrapper import register_method
from pandas import DataFrame, Series, concat

from sklearn.linear_model import LinearRegression, Ridge, Lasso, SGDRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import (
    VotingRegressor,
    BaggingRegressor,
    RandomForestRegressor,
    AdaBoostRegressor,
    GradientBoostingRegressor,
)
from lightgbm import LGBMRegressor
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.stattools import durbin_watson
from statsmodels.stats.api import het_breuschpagan
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from xgboost import XGBRegressor
from scipy.stats import t, f
from .util import my_pretty_table, my_trend, my_train_test_split
from .plot import my_residplot, my_qqplot, my_learing_curve, my_barplot
from .core import __ml, get_estimator, get_hyper_params

@register_method
def __my_regression(
    classname: any,
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = True,
    resid_test=True,
    deg: int = 1,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    is_print: bool = True,
    est: any = None,
    **params,
) -> any:
    """íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        classname (any): íšŒê·€ë¶„ì„ ì¶”ì •ê¸° (ëª¨ë¸ ê°ì²´)
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
        est (any, optional): Voting, Bagging ì•™ìƒë¸” ëª¨ë¸ì˜ ê¸°ë³¸ ì¶”ì •ê¸°. Defaults to None.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        any: íšŒê·€ë¶„ì„ ëª¨ë¸
    """

    # ------------------------------------------------------
    # ë¶„ì„ëª¨ë¸ ìƒì„±
    estimator = __ml(
        classname=classname,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        is_print=is_print,
        est=est,
        **params,
    )

    if estimator is None:
        print(f"\033[91m[{classname} ëª¨ë¸ì˜ í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\033[0m")
        return None

    # ------------------------------------------------------
    # ì„±ëŠ¥í‰ê°€
    my_regression_result(
        estimator=estimator,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        learning_curve=learning_curve,
        cv=cv,
        figsize=figsize,
        dpi=dpi,
        is_print=is_print,
    )

    # ------------------------------------------------------
    # ë³´ê³ ì„œ ì¶œë ¥
    if report and is_print:
        print("")
        my_regression_report(
            estimator=estimator,
            x_train=estimator.x,
            y_train=estimator.y,
            x_test=sort,
            plot=plot,
            deg=deg,
            figsize=figsize,
            dpi=dpi,
        )

    # ------------------------------------------------------
    # ì”ì°¨ ê°€ì • í™•ì¸
    if resid_test and is_print:
        print("\n\n[ì”ì°¨ì˜ ê°€ì • í™•ì¸] ==============================")
        my_resid_test(
            x=estimator.x, y=estimator.y, y_pred=estimator.y_pred, figsize=figsize, dpi=dpi
        )

    return estimator

@register_method
def my_auto_linear_regression(df:DataFrame, yname:str, cv:int=5, learning_curve: bool = True, degree : int = 1, plot: bool = True, report=True, resid_test=False, figsize=(10, 4), dpi=150, sort: str = None,order: str = None,p_value_num:float=0.05) -> LinearRegression:
    """ì„ í˜•íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        df (DataFrame) : íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•  ë°ì´í„°í”„ë ˆì„.
        yname (str) : ì¢…ì†ë³€ìˆ˜
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 4).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 150.
        order (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        p_value_num (float, optional) : íšŒê·€ëª¨í˜•ì˜ ìœ ì˜í™•ë¥ . Drfaults to 0.05
    Returns:
        LinearRegression: íšŒê·€ë¶„ì„ ëª¨ë¸
    """

    x_train, x_test, y_train, y_test = my_train_test_split(df, yname, test_size=0.2)
    
    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    # ë¶„ì„ëª¨ë¸ ìƒì„±
    model = LinearRegression(n_jobs=-1) # n_jobs : ì‚¬ìš©í•˜ëŠ” cpu ì½”ì–´ì˜ ê°œìˆ˜ // -1ì€ ìµœëŒ€ì¹˜

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        params = {}
        grid = GridSearchCV(model, param_grid=params, cv=cv, n_jobs=-1)
        fit = grid.fit(x_train, y_train)
        model = fit.best_estimator_
        fit.best_params = fit.best_params_
        
        result_df = DataFrame(grid.cv_results_['params'])
        result_df['mean_test_score'] = grid.cv_results_['mean_test_score']
        
        # print("[êµì°¨ê²€ì¦]")
        # my_pretty_table(result_df.sort_values(by='mean_test_score', ascending=False))
        # print("")

    fit = model.fit(x_train, y_train)
    x = x_test
    y = y_test
    y_pred = fit.predict(x)

    resid = y - y_pred

    # ì ˆí¸ê³¼ ê³„ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
    params = np.append(fit.intercept_, fit.coef_)

    # ê²€ì¦ìš© ë…ë¦½ë³€ìˆ˜ì— ìƒìˆ˜í•­ ì¶”ê°€
    design_x = x.copy()
    design_x.insert(0, 'ìƒìˆ˜', 1)

    dot = np.dot(design_x.T,design_x)   # í–‰ë ¬ê³±
    inv = np.linalg.inv(dot)            # ì—­í–‰ë ¬
    dia = inv.diagonal()                # ëŒ€ê°ì›ì†Œ

    # ì œê³±ì˜¤ì°¨
    MSE = (sum((y-y_pred)**2)) / (len(design_x)-len(design_x.iloc[0]))

    se_b = np.sqrt(MSE * dia)           # í‘œì¤€ì˜¤ì°¨
    ts_b = params / se_b                # tê°’

    # ê° ë…ë¦½ìˆ˜ì— ëŒ€í•œ pvalue
    p_values = [2*(1-t.cdf(np.abs(i),(len(design_x)-len(design_x.iloc[0])))) for i in set(ts_b)]

    # VIF
    if len(x.columns) > 1:
        vif = [variance_inflation_factor(x, list(x.columns).index(v)) for  v in set(x.columns)]
    else:
        vif = 0

    # í‘œì¤€í™” ê³„ìˆ˜
    train_df = x.copy()
    train_df[y.name] = y
    scaler = StandardScaler()
    std = scaler.fit_transform(train_df)
    std_df = DataFrame(std, columns=train_df.columns)
    std_x = std_df[xnames]
    std_y = std_df[yname]
    std_model = LinearRegression()
    std_fit = std_model.fit(std_x, std_y)
    beta = std_fit.coef_

    # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
    result_df = DataFrame({
        "ì¢…ì†ë³€ìˆ˜": [yname] * len(xnames),
        "ë…ë¦½ë³€ìˆ˜": xnames,
        "B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)": np.round(params[1:], 4),
        "í‘œì¤€ì˜¤ì°¨": np.round(se_b[1:], 3),
        "Î²(í‘œì¤€í™” ê³„ìˆ˜)": np.round(beta, 3),
        "t": np.round(ts_b[1:], 3),
        "ìœ ì˜í™•ë¥ ": np.round(p_values[1:], 3),
        "VIF": vif,
    })
    if order:
        order = order.upper()
        if order == 'V':
            result_df.sort_values('VIF',inplace=True)
        elif  order == 'P':
            result_df.sort_values('ìœ ì˜í™•ë¥ ',inplace=True)
        #result_df
    # my_pretty_table(result_df)
        
    resid = y - y_pred        # ì”ì°¨
    dw = durbin_watson(resid)               # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰
    r2 = r2_score(y, y_pred)  # ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥)
    rowcount = len(x)                # í‘œë³¸ìˆ˜
    featurecount = len(x.columns)    # ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜

    # ë³´ì •ëœ ê²°ì •ê³„ìˆ˜
    adj_r2 = 1 - (1 - r2) * (rowcount-1) / (rowcount-featurecount-1)

    # fê°’
    f_statistic = (r2 / featurecount) / ((1 - r2) / (rowcount - featurecount - 1))

    # Prob (F-statistic)
    p = 1 - f.cdf(f_statistic, featurecount, rowcount - featurecount - 1)

    tpl = f"ğ‘…^2({r2:.3f}), Adj.ğ‘…^2({adj_r2:.3f}), F({f_statistic:.3f}), P-value({p:.4g}), Durbin-Watson({dw:.3f})"
    # print(tpl, end="\n\n")

    # ê²°ê³¼ë³´ê³ 
    tpl = f"{yname}ì— ëŒ€í•˜ì—¬ {','.join(xnames)}ë¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜{'í•˜ë‹¤' if p <= 0.05 else 'í•˜ì§€ ì•Šë‹¤'}(F({len(x.columns)},{len(x.index)-len(x.columns)-1}) = {f_statistic:0.3f}, p {'<=' if p <= p_value_num else '>'} 0.05)."

    # # print(tpl, end = '\n\n')

    # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
    for n in xnames:
        item = result_df[result_df['ë…ë¦½ë³€ìˆ˜'] == n]
        coef = item['B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)'].values[0]
        pvalue = item['ìœ ì˜í™•ë¥ '].values[0]

        s = f"{n}ì˜ íšŒê·€ê³„ìˆ˜ëŠ” {coef:0.3f}(p {'<=' if pvalue <= p_value_num else '>'} 0.05)ë¡œ, {yname}ì— ëŒ€í•˜ì—¬ {'ìœ ì˜ë¯¸í•œ' if pvalue <= p_value_num else 'ìœ ì˜í•˜ì§€ ì•Šì€'} ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤."

        # print(s)
        
    # print("")
    if result_df["VIF"].max() >= 10:
        # print('-'*50)
        # print('ëº€ ë³€ìˆ˜ :',result_df['ë…ë¦½ë³€ìˆ˜'][result_df['VIF'].idxmax()])
        # print('-'*50)
        return my_auto_linear_regression(df.drop(result_df['ë…ë¦½ë³€ìˆ˜'][result_df['VIF'].idxmax()],axis=1), yname, cv, degree,plot,report,resid_test, figsize, dpi, order,p_value_num )
    else:
        if result_df["ìœ ì˜í™•ë¥ "].max() >= p_value_num:
            # print('-'*50)
            # print('ëº€ ë³€ìˆ˜ :',result_df['ë…ë¦½ë³€ìˆ˜'][result_df['ìœ ì˜í™•ë¥ '].idxmax()])
            # print('-'*50)
            return my_auto_linear_regression(df.drop(result_df['ë…ë¦½ë³€ìˆ˜'][result_df['ìœ ì˜í™•ë¥ '].idxmax()],axis=1), yname,cv, degree,plot,report,resid_test, figsize, dpi, order,p_value_num )
    
    x_train, x_test, y_train, y_test = my_train_test_split(df, yname, test_size=0.2)
    
    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    # ë¶„ì„ëª¨ë¸ ìƒì„±
    model = LinearRegression(n_jobs=-1) # n_jobs : ì‚¬ìš©í•˜ëŠ” cpu ì½”ì–´ì˜ ê°œìˆ˜ // -1ì€ ìµœëŒ€ì¹˜

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        params = {}
        grid = GridSearchCV(model, param_grid=params, cv=cv, n_jobs=-1)
        fit = grid.fit(x_train, y_train)
        model = fit.best_estimator_
        fit.best_params = fit.best_params_
        
        result_df = DataFrame(grid.cv_results_['params'])
        result_df['mean_test_score'] = grid.cv_results_['mean_test_score']
        
        print("[êµì°¨ê²€ì¦]")
        my_pretty_table(result_df.sort_values(by='mean_test_score', ascending=False))
        print("")

    fit = model.fit(x_train, y_train)
    x = x_test
    y = y_test
    y_pred = fit.predict(x)
    expr = "{yname} = ".format(yname=yname)

    for i, v in enumerate(xnames):
        expr += "%0.3f * %s + " % (fit.coef_[i], v)

    expr += "%0.3f" % fit.intercept_
    print("[íšŒê·€ì‹]")
    print(expr, end="\n\n")
    resid = y - y_pred

    # ì ˆí¸ê³¼ ê³„ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
    params = np.append(fit.intercept_, fit.coef_)

    # ê²€ì¦ìš© ë…ë¦½ë³€ìˆ˜ì— ìƒìˆ˜í•­ ì¶”ê°€
    design_x = x.copy()
    design_x.insert(0, 'ìƒìˆ˜', 1)

    dot = np.dot(design_x.T,design_x)   # í–‰ë ¬ê³±
    inv = np.linalg.inv(dot)            # ì—­í–‰ë ¬
    dia = inv.diagonal()                # ëŒ€ê°ì›ì†Œ

    # ì œê³±ì˜¤ì°¨
    MSE = (sum((y-y_pred)**2)) / (len(design_x)-len(design_x.iloc[0]))

    se_b = np.sqrt(MSE * dia)           # í‘œì¤€ì˜¤ì°¨
    ts_b = params / se_b                # tê°’

    # ê° ë…ë¦½ìˆ˜ì— ëŒ€í•œ pvalue
    p_values = [2*(1-t.cdf(np.abs(i),(len(design_x)-len(design_x.iloc[0])))) for i in set(ts_b)]

    # VIF
    if len(x.columns) > 1:
        vif = [variance_inflation_factor(x, list(x.columns).index(v)) for  v in set(x.columns)]
    else:
        vif = 0

    # í‘œì¤€í™” ê³„ìˆ˜
    train_df = x.copy()
    train_df[y.name] = y
    scaler = StandardScaler()
    std = scaler.fit_transform(train_df)
    std_df = DataFrame(std, columns=train_df.columns)
    std_x = std_df[xnames]
    std_y = std_df[yname]
    std_model = LinearRegression()
    std_fit = std_model.fit(std_x, std_y)
    beta = std_fit.coef_

    # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
    result_df = DataFrame({
        "ì¢…ì†ë³€ìˆ˜": [yname] * len(xnames),
        "ë…ë¦½ë³€ìˆ˜": xnames,
        "B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)": np.round(params[1:], 4),
        "í‘œì¤€ì˜¤ì°¨": np.round(se_b[1:], 3),
        "Î²(í‘œì¤€í™” ê³„ìˆ˜)": np.round(beta, 3),
        "t": np.round(ts_b[1:], 3),
        "ìœ ì˜í™•ë¥ ": np.round(p_values[1:], 3),
        "VIF": vif,
    })
    if order:
        order = order.upper()
        if order == 'V':
            result_df.sort_values('VIF',inplace=True)
        elif  order == 'P':
            result_df.sort_values('ìœ ì˜í™•ë¥ ',inplace=True)
        # result_df
    my_pretty_table(result_df)
        
    resid = y - y_pred        # ì”ì°¨
    dw = durbin_watson(resid)               # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰
    r2 = r2_score(y, y_pred)  # ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥)
    rowcount = len(x)                # í‘œë³¸ìˆ˜
    featurecount = len(x.columns)    # ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜

    # ë³´ì •ëœ ê²°ì •ê³„ìˆ˜
    adj_r2 = 1 - (1 - r2) * (rowcount-1) / (rowcount-featurecount-1)

    # fê°’
    f_statistic = (r2 / featurecount) / ((1 - r2) / (rowcount - featurecount - 1))

    # Prob (F-statistic)
    p = 1 - f.cdf(f_statistic, featurecount, rowcount - featurecount - 1)

    tpl = f"ğ‘…^2({r2:.3f}), Adj.ğ‘…^2({adj_r2:.3f}), F({f_statistic:.3f}), P-value({p:.4g}), Durbin-Watson({dw:.3f})"
    print(tpl, end="\n\n")

    # ê²°ê³¼ë³´ê³ 
    tpl = f"{yname}ì— ëŒ€í•˜ì—¬ {','.join(xnames)}ë¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜{'í•˜ë‹¤' if p <= 0.05 else 'í•˜ì§€ ì•Šë‹¤'}(F({len(x.columns)},{len(x.index)-len(x.columns)-1}) = {f_statistic:0.3f}, p {'<=' if p <= p_value_num else '>'} 0.05)."

    print(tpl, end = '\n\n')

    # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
    for n in xnames:
        item = result_df[result_df['ë…ë¦½ë³€ìˆ˜'] == n]
        coef = item['B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)'].values[0]
        pvalue = item['ìœ ì˜í™•ë¥ '].values[0]

        s = f"{n}ì˜ íšŒê·€ê³„ìˆ˜ëŠ” {coef:0.3f}(p {'<=' if pvalue <= p_value_num else '>'} 0.05)ë¡œ, {yname}ì— ëŒ€í•˜ì—¬ {'ìœ ì˜ë¯¸í•œ' if pvalue <= p_value_num else 'ìœ ì˜í•˜ì§€ ì•Šì€'} ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤."

        print(s)
        
    print("")
    return fit
    
@register_method
def my_linear_regression(x_train: DataFrame, y_train: Series, x_test: DataFrame = None, y_test: Series = None, cv: int = 5,  learning_curve: bool = True, deg:int = 1,degree : int = 1, plot: bool = True, is_print:bool = True,report=True, resid_test=False, figsize=(10, 4), dpi=150, sort: str = None,order: str = None,p_value_num:float=0.05, **params ) -> LinearRegression:
    """ì„ í˜•íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.
        order (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        p_value_num (float, optional) : íšŒê·€ëª¨í˜•ì˜ ìœ ì˜í™•ë¥ . Drfaults to 0.05
    Returns:
        LinearRegression: íšŒê·€ë¶„ì„ ëª¨ë¸
    """
    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    # ë¶„ì„ëª¨ë¸ ìƒì„±

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = get_hyper_params(LinearRegression)
        
    return __my_regression(
        classname=LinearRegression,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        **params,
    )

@register_method
def my_ridge_regression(x_train: DataFrame, y_train: Series, x_test: DataFrame = None, y_test: Series = None, cv: int = 5, learning_curve: bool = True, report=False, plot: bool = False, degree: int = 1, resid_test=False, figsize=(10, 5), dpi: int = 100, sort: str = None, params: dict = {'alpha': [0.01, 0.1, 1, 10, 100]}) -> LinearRegression:
    """ë¦¿ì§€íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to {'alpha': [0.01, 0.1, 1, 10, 100]}.
        
    Returns:
        Ridge: Ridge ëª¨ë¸
    """
    
    #------------------------------------------------------
    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = get_hyper_params(classname=Ridge)

    return __my_regression(
        classname=Ridge,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        **params,
    )

@register_method
def my_lasso_regression(    
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = False,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    is_print: bool = True,
    **params)-> Lasso:
    """ë¼ì˜íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to {'alpha': [0.01, 0.1, 1, 10, 100]}.
        
    Returns:
        Lasso: Lasso ëª¨ë¸
    """
    
    #------------------------------------------------------
    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = get_hyper_params(classname=Lasso)

    return __my_regression(
        classname=Lasso,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        **params,
    )

@register_method
def __regression_report_plot(ax: plt.Axes, x, y, xname, yname, y_pred, deg) -> None:
    if deg == 1:
        sb.regplot(x=x, y=y, ci=95, label="ê´€ì¸¡ì¹˜", ax=ax)
        sb.regplot(x=x, y=y_pred, ci=0, label="ì¶”ì •ì¹˜", ax=ax)
    else:
        sb.scatterplot(x=x, y=y, label="ê´€ì¸¡ì¹˜", ax=ax)
        sb.scatterplot(x=x, y=y_pred, label="ì¶”ì •ì¹˜", ax=ax)

        t1 = my_trend(x, y, degree=deg)
        sb.lineplot(
            x=t1[0], y=t1[1], color="blue", linestyle="--", label="ê´€ì¸¡ì¹˜ ì¶”ì„¸ì„ ", ax=ax
        )

        t2 = my_trend(x, y_pred, degree=deg)
        sb.lineplot(
            x=t2[0], y=t2[1], color="red", linestyle="--", label="ì¶”ì •ì¹˜ ì¶”ì„¸ì„ ", ax=ax
        )

        ax.set_xlabel(xname)
        ax.set_ylabel(yname)

    ax.legend()
    ax.grid()

@register_method
def my_regression_result(
    estimator: any,
    x_train: DataFrame = None,
    y_train: Series = None,
    x_test: DataFrame = None,
    y_test: Series = None,
    learning_curve: bool = True,
    cv: int = 10,
    figsize: tuple = (10, 5),
    dpi: int = 100,
    is_print: bool = True,
) -> None:
    """íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        estimator (any): íšŒê·€ë¶„ì„ ëª¨ë¸
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 10.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
    """

    scores = []
    score_names = []

    if x_train is not None and y_train is not None:
        y_train_pred = estimator.predict(x_train)

        # ì„±ëŠ¥í‰ê°€
        result = {
            "ê²°ì •ê³„ìˆ˜(R2)": r2_score(y_train, y_train_pred),
            "í‰ê· ì ˆëŒ€ì˜¤ì°¨(MAE)": mean_absolute_error(y_train, y_train_pred),
            "í‰ê· ì œê³±ì˜¤ì°¨(MSE)": mean_squared_error(y_train, y_train_pred),
            "í‰ê· ì˜¤ì°¨(RMSE)": np.sqrt(mean_squared_error(y_train, y_train_pred)),
            "í‰ê·  ì ˆëŒ€ ë°±ë¶„ì˜¤ì°¨ ë¹„ìœ¨(MAPE)": np.mean(
                np.abs((y_train - y_train_pred) / y_train) * 100
            ),
            "í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨(MPE)": np.mean((y_train - y_train_pred) / y_train * 100),
        }

        scores.append(result)
        score_names.append("í›ˆë ¨ë°ì´í„°")

    if x_test is not None and y_test is not None:
        y_test_pred = estimator.predict(x_test)

        # ì„±ëŠ¥í‰ê°€
        result = {
            "ê²°ì •ê³„ìˆ˜(R2)": r2_score(y_test, y_test_pred),
            "í‰ê· ì ˆëŒ€ì˜¤ì°¨(MAE)": mean_absolute_error(y_test, y_test_pred),
            "í‰ê· ì œê³±ì˜¤ì°¨(MSE)": mean_squared_error(y_test, y_test_pred),
            "í‰ê· ì˜¤ì°¨(RMSE)": np.sqrt(mean_squared_error(y_test, y_test_pred)),
            "í‰ê·  ì ˆëŒ€ ë°±ë¶„ì˜¤ì°¨ ë¹„ìœ¨(MAPE)": np.mean(
                np.abs((y_test - y_test_pred) / y_test) * 100
            ),
            "í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨(MPE)": np.mean((y_test - y_test_pred) / y_test * 100),
        }

        scores.append(result)
        score_names.append("ê²€ì¦ë°ì´í„°")

    # ê²°ê³¼ê°’ì„ ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´
    estimator.scores = scores[-1]

    # ------------------------------------------------------
    if is_print:
        print("[íšŒê·€ë¶„ì„ ì„±ëŠ¥í‰ê°€]")
        result_df = DataFrame(scores, index=score_names)
        my_pretty_table(result_df.T)

        # í•™ìŠµê³¡ì„ 
        if learning_curve:
            print("\n[í•™ìŠµê³¡ì„ ]")
            yname = y_train.name

            if x_test is not None and y_test is not None:
                y_df = concat([y_train, y_test])
                x_df = concat([x_train, x_test])
            else:
                y_df = y_train.copy()
                x_df = x_train.copy()

            x_df[yname] = y_df
            x_df.sort_index(inplace=True)

            if cv > 0:
                my_learing_curve(
                    estimator,
                    data=x_df,
                    yname=yname,
                    cv=cv,
                    scoring="RMSE",
                    figsize=figsize,
                    dpi=dpi,
                )
            else:
                my_learing_curve(
                    estimator,
                    data=x_df,
                    yname=yname,
                    scoring="RMSE",
                    figsize=figsize,
                    dpi=dpi,
                )

        if estimator.__class__.__name__ == "XGBRegressor":
            print("\n[ë³€ìˆ˜ ì¤‘ìš”ë„]")
            my_plot_importance(estimator=estimator)

            feature_important = estimator.get_booster().get_score(
                importance_type="weight"
            )
            keys = list(feature_important.keys())
            values = list(feature_important.values())

            data = DataFrame(data=values, index=keys, columns=["score"]).sort_values(
                by="score", ascending=False
            )

            data["rate"] = data["score"] / data["score"].sum()
            data["cumsum"] = data["rate"].cumsum()

            my_pretty_table(data)

            # print("\n[TREE]")
            # my_xgb_tree(booster=estimator)


@register_method
def my_regression_report(
    estimator: any,
    x_train: DataFrame = None,
    y_train: Series = None,
    x_test: DataFrame = None,
    y_test: Series = None,
    sort: str = None,
    plot: bool = False,
    deg: int = 1,
    figsize: tuple = (10, 5),
    dpi: int = 100,
) -> None:
    """ì„ í˜•íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ í•œë‹¤.

    Args:
        estimator (LinearRegression): ì„ í˜•íšŒê·€ ê°ì²´
        x_train (DataFrame, optional): í›ˆë ¨ ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜. Defaults to None.
        y_train (Series, optional): í›ˆë ¨ ë°ì´í„°ì˜ ì¢…ì†ë³€ìˆ˜. Defaults to None.
        x_test (DataFrame, optional): ê²€ì¦ ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜. Defaults to None.
        y_test (Series, optional): ê²€ì¦ ë°ì´í„°ì˜ ì¢…ì†ë³€ìˆ˜. Defaults to None.
        sort (str, optional): ì •ë ¬ ê¸°ì¤€ (v, p). Defaults to None.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to False.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
    """

    # ------------------------------------------------------
    # íšŒê·€ì‹

    if x_test is not None and y_test is not None:
        x = x_test.copy()
        y = y_test.copy()
    else:
        x = x_train.copy()
        y = y_train.copy()

    xnames = x.columns
    yname = y.name
    y_pred = estimator.predict(x)

    if estimator.__class__.__name__ in ["LinearRegression", "Lasso", "Ridge"]:
        expr = "{yname} = ".format(yname=yname)

        for i, v in enumerate(xnames):
            expr += "%0.3f * %s + " % (estimator.coef_[i], v)

        expr += "%0.3f" % estimator.intercept_
        print("[íšŒê·€ì‹]")
        print(expr, end="\n\n")

        print("[ë…ë¦½ë³€ìˆ˜ë³´ê³ ]")

        if x is None and y is None:
            x = estimator.x
            y = estimator.y

        # ì”ì°¨
        resid = y - y_pred

        # ì ˆí¸ê³¼ ê³„ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
        params = np.append(estimator.intercept_, estimator.coef_)

        # ê²€ì¦ìš© ë…ë¦½ë³€ìˆ˜ì— ìƒìˆ˜í•­ ì¶”ê°€
        design_x = x.copy()
        design_x.insert(0, "ìƒìˆ˜", 1)

        dot = np.dot(design_x.T, design_x)  # í–‰ë ¬ê³±
        inv = np.linalg.inv(dot)  # ì—­í–‰ë ¬
        dia = inv.diagonal()  # ëŒ€ê°ì›ì†Œ

        # ì œê³±ì˜¤ì°¨
        MSE = (sum((y - y_pred) ** 2)) / (len(design_x) - len(design_x.iloc[0]))

        se_b = np.sqrt(MSE * dia)  # í‘œì¤€ì˜¤ì°¨
        ts_b = params / se_b  # tê°’

        # ê° ë…ë¦½ìˆ˜ì— ëŒ€í•œ pvalue
        p_values = [
            2 * (1 - t.cdf(np.abs(i), (len(design_x) - len(design_x.iloc[0]))))
            for i in ts_b
        ]

        # VIF
        if len(x.columns) > 1:
            vif = [
                variance_inflation_factor(x, list(x.columns).index(v))
                for i, v in enumerate(x.columns)
            ]
        else:
            vif = 0

        # í‘œì¤€í™” ê³„ìˆ˜
        train_df = x.copy()
        train_df[y.name] = y
        scaler = StandardScaler()
        std = scaler.fit_transform(train_df)
        std_df = DataFrame(std, columns=train_df.columns)
        std_x = std_df[xnames]
        std_y = std_df[yname]
        std_estimator = LinearRegression(n_jobs=-1)
        std_estimator.fit(std_x, std_y)
        beta = std_estimator.coef_

        # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
        result_df = DataFrame(
            {
                "ì¢…ì†ë³€ìˆ˜": [yname] * len(xnames),
                "ë…ë¦½ë³€ìˆ˜": xnames,
                "B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)": np.round(params[1:], 4),
                "í‘œì¤€ì˜¤ì°¨": np.round(se_b[1:], 3),
                "Î²(í‘œì¤€í™” ê³„ìˆ˜)": np.round(beta, 3),
                "t": np.round(ts_b[1:], 3),
                "ìœ ì˜í™•ë¥ ": np.round(p_values[1:], 3),
                "VIF": vif,
            }
        )

        if sort:
            if sort.upper() == "V":
                result_df = result_df.sort_values("VIF", ascending=False).reset_index()
            elif sort.upper() == "P":
                result_df = result_df.sort_values(
                    "ìœ ì˜í™•ë¥ ", ascending=False
                ).reset_index()

        # result_df
        my_pretty_table(result_df)
        print("")

        resid = y - y_pred  # ì”ì°¨
        dw = durbin_watson(resid)  # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰
        r2 = r2_score(y, y_pred)  # ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥)
        rowcount = len(x)  # í‘œë³¸ìˆ˜
        featurecount = len(x.columns)  # ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜

        # ë³´ì •ëœ ê²°ì •ê³„ìˆ˜
        adj_r2 = 1 - (1 - r2) * (rowcount - 1) / (rowcount - featurecount - 1)

        # fê°’
        f_statistic = (r2 / featurecount) / ((1 - r2) / (rowcount - featurecount - 1))

        # Prob (F-statistic)
        p = 1 - f.cdf(f_statistic, featurecount, rowcount - featurecount - 1)

        tpl = "ğ‘…^2(%.3f), Adj.ğ‘…^2(%.3f), F(%.3f), P-value(%.4g), Durbin-Watson(%.3f)"
        print(tpl % (r2, adj_r2, f_statistic, p, dw), end="\n\n")

        # ê²°ê³¼ë³´ê³ 
        tpl = "%sì— ëŒ€í•˜ì—¬ %së¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼,\nì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ %s(F(%s,%s) = %0.3f, p %s 0.05)."

        result_str = tpl % (
            yname,
            ",".join(xnames),
            "ìœ ì˜í•˜ë‹¤" if p <= 0.05 else "ìœ ì˜í•˜ì§€ ì•Šë‹¤",
            len(x.columns),
            len(x.index) - len(x.columns) - 1,
            f_statistic,
            "<=" if p <= 0.05 else ">",
        )

        print(result_str, end="\n\n")

        # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
        for n in xnames:
            item = result_df[result_df["ë…ë¦½ë³€ìˆ˜"] == n]
            coef = item["B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)"].values[0]
            pvalue = item["ìœ ì˜í™•ë¥ "].values[0]

            s = "%sì˜ íšŒê·€ê³„ìˆ˜ëŠ” %0.3f(p %s 0.05)ë¡œ, %sì— ëŒ€í•˜ì—¬ %s."
            k = s % (
                n,
                coef,
                "<=" if pvalue <= 0.05 else ">",
                yname,
                (
                    "ìœ ì˜ë¯¸í•œ ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤"
                    if pvalue <= 0.05
                    else "ìœ ì˜í•˜ì§€ ì•Šì€ ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤"
                ),
            )

            print(k)

        # ë„ì¶œëœ ê²°ê³¼ë¥¼ íšŒê·€ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´ --> ê°ì²´ íƒ€ì…ì˜ íŒŒë¼ë¯¸í„°ëŠ” ì°¸ì¡°ë³€ìˆ˜ë¡œ ì „ë‹¬ë˜ë¯€ë¡œ fit ê°ì²´ì— í¬í•¨ëœ ê²°ê³¼ê°’ë“¤ì€ ì´ í•¨ìˆ˜ ì™¸ë¶€ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.
        estimator.r2 = r2
        estimator.adj_r2 = adj_r2
        estimator.f_statistic = f_statistic
        estimator.p = p
        estimator.dw = dw

    else:
        # VIF
        if len(x.columns) > 1:
            vif = [
                variance_inflation_factor(x, list(x.columns).index(v))
                for i, v in enumerate(x.columns)
            ]
        else:
            vif = 0

        # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
        result_df = DataFrame(
            {
                "ì¢…ì†ë³€ìˆ˜": [yname] * len(xnames),
                "ë…ë¦½ë³€ìˆ˜": xnames,
                "VIF": vif,
            }
        )

        if sort:
            if sort.upper() == "V":
                result_df = result_df.sort_values("VIF", ascending=False).reset_index()

        # result_df
        my_pretty_table(result_df)
        print("")

    # ì‹œê°í™”
    if plot:
        size = len(xnames)
        cols = 2
        rows = (size + cols - 1) // cols

        fig, ax = plt.subplots(
            nrows=rows,
            ncols=cols,
            squeeze=False,
            figsize=(figsize[0] * cols, figsize[1] * rows),
            dpi=dpi,
        )

        fig.subplots_adjust(wspace=0.1, hspace=0.3)

        with futures.ThreadPoolExecutor() as executor:
            for i, v in enumerate(xnames):
                r = i // cols
                c = i % cols

                executor.submit(
                    __regression_report_plot,
                    ax=ax[r, c],
                    x=x[v],
                    y=y,
                    xname=v,
                    yname=yname,
                    y_pred=y_pred,
                    deg=deg,
                )

        plt.show()
        plt.close()

@register_method
def my_resid_normality(y: Series, y_pred: Series) -> None:
    """MSEê°’ì„ ì´ìš©í•˜ì—¬ ì”ì°¨ì˜ ì •ê·œì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
    """
    mse = mean_squared_error(y, y_pred)
    resid = y - y_pred
    mse_sq = np.sqrt(mse)

    r1 = resid[ (resid > -mse_sq) & (resid < mse_sq)].count() / resid.count() * 100
    r2 = resid[ (resid > -2*mse_sq) & (resid < 2*mse_sq)].count() / resid.count() * 100
    r3 = resid[ (resid > -3*mse_sq) & (resid < 3*mse_sq)].count() / resid.count() * 100

    mse_r = [r1, r2, r3]
    
    print(f"ë£¨íŠ¸ 1MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r1:1.2f}% ({r1-68})")
    print(f"ë£¨íŠ¸ 2MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r2:1.2f}% ({r2-95})")
    print(f"ë£¨íŠ¸ 3MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r3:1.2f}% ({r3-99})")
    
    normality = r1 >= 68 and r2 >= 95 and r3 >= 99
    print(f"ì”ì°¨ì˜ ì •ê·œì„± ê°€ì • ì¶©ì¡± ì—¬ë¶€: {normality}")

@register_method
def my_resid_equal_var(x: DataFrame, y: Series, y_pred: Series, p_value_num:float =0.05) -> None:
    """ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        x (DataFrame): ë…ë¦½ë³€ìˆ˜
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
        p_value_num(float) : ìœ ì˜í™•ë¥ 
    """
    # ë…ë¦½ë³€ìˆ˜ ë°ì´í„° í”„ë ˆì„ ë³µì‚¬
    x_copy = x.copy()
    
    # ìƒìˆ˜í•­ ì¶”ê°€
    x_copy.insert(0, "const", 1)
    
    # ì”ì°¨ êµ¬í•˜ê¸°
    resid = y - y_pred
    
    # ë“±ë¶„ì‚°ì„± ê²€ì •
    bs_result = het_breuschpagan(resid, x_copy)
    bs_result_df = DataFrame(bs_result, columns=['values'], index=['statistic', 'p-value', 'f-value', 'f p-value'])

    print(f"ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì • ì¶©ì¡± ì—¬ë¶€: {bs_result[1] > p_value_num}")
    my_pretty_table(bs_result_df)

@register_method
def my_resid_independence(y: Series, y_pred: Series) -> None:
    """ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
    """
    dw = durbin_watson(y - y_pred)
    print(f"Durbin-Watson: {dw}, ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì • ë§Œì¡± ì—¬ë¶€: {1.5 < dw < 2.5}")
    
@register_method    
def my_resid_test(x: DataFrame, y: Series, y_pred: Series, figsize: tuple=(10, 4), dpi: int=150, p_value_num:float = 0.05) -> None:
    """ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        x (Series): ë…ë¦½ë³€ìˆ˜
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
        p_value_num(float) : ìœ ì˜í™•ë¥ 
    """

    # ì”ì°¨ ìƒì„±
    resid = y - y_pred
    
    print("[ì”ì°¨ì˜ ì„ í˜•ì„± ê°€ì •]")
    my_residplot(y, y_pred, lowess=True, figsize=figsize, dpi=dpi)
    
    print("\n[ì”ì°¨ì˜ ì •ê·œì„± ê°€ì •]")
    my_qqplot(y, figsize=figsize, dpi=dpi)
    my_residplot(y, y_pred, mse=True, figsize=figsize, dpi=dpi)
    my_resid_normality(y, y_pred)
    
    print("\n[ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì •]")
    my_resid_equal_var(x, y, y_pred, p_value_num)
    
    print("\n[ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì •]")
    my_resid_independence(y, y_pred)

@register_method
def my_ridge_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = False,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    is_print: bool = True,
    **params,
) -> Ridge:
    """ë¦¿ì§€íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        Ridge: Ridge ëª¨ë¸
    """

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = {"alpha": [0.01, 0.1, 1, 10, 100]}

    return __my_regression(
        classname=Ridge,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        **params,
    )

@register_method
def my_lasso_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = False,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    is_print: bool = True,
    **params,
) -> Lasso:
    """ë¼ì˜íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        Lasso: Lasso ëª¨ë¸
    """

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = {"alpha": [0.01, 0.1, 1, 10, 100]}

    return __my_regression(
        classname=Lasso,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        **params,
    )

@register_method
def my_knn_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = False,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    is_print: bool = True,
    **params,
) -> KNeighborsRegressor:
    """KNN íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        KNeighborsRegressor
    """

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = get_hyper_params(classname=KNeighborsRegressor)
    return __my_regression(
        classname=KNeighborsRegressor,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        **params,
    )

@register_method
def my_dtree_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    pruning: bool = False,
    learning_curve: bool = True,
    report=True,
    plot: bool = False,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    is_print: bool = True,
    **params,
) -> DecisionTreeRegressor:
    """DecisionTree íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
        pruning (bool, optional): ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì—ì„œ ê°€ì§€ì¹˜ê¸°ì˜ alphaê°’ì„ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì— í¬í•¨ í• ì§€ ì—¬ë¶€. Default to False.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        DecisionTreeRegressor
    """

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = get_hyper_params(classname=DecisionTreeRegressor)

        if pruning:
            print("\033[91mê°€ì§€ì¹˜ê¸°ë¥¼ ìœ„í•œ alphaê°’ì„ íƒìƒ‰í•©ë‹ˆë‹¤.\033[0m")

            try:
                dtree = get_estimator(classname=DecisionTreeRegressor)
                path = dtree.cost_complexity_pruning_path(x_train, y_train)
                ccp_alphas = path.ccp_alphas[1:-1]
                params["ccp_alpha"] = ccp_alphas
            except Exception as e:
                print(f"\033[91mê°€ì§€ì¹˜ê¸° ì‹¤íŒ¨ ({e})\033[0m")
                e.with_traceback()
        else:
            if "ccp_alpha" in params:
                del params["ccp_alpha"]

            print("\033[91mê°€ì§€ì¹˜ê¸°ë¥¼ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\033[0m")

    return __my_regression(
        classname=DecisionTreeRegressor,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        pruning=pruning,
        **params,
    )

@register_method
def my_svr_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = False,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    is_print: bool = True,
    **params,
) -> SVR:
    """Support Vector Machine íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
        pruning (bool, optional): ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì—ì„œ ê°€ì§€ì¹˜ê¸°ì˜ alphaê°’ì„ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì— í¬í•¨ í• ì§€ ì—¬ë¶€. Default to False.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        SVR
    """

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = get_hyper_params(classname=SVR)
    return __my_regression(
        classname=SVR,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        **params,
    )

@register_method
def my_sgd_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = False,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    is_print: bool = True,
    **params,
) -> SGDRegressor:
    """SGD íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
        pruning (bool, optional): ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì—ì„œ ê°€ì§€ì¹˜ê¸°ì˜ alphaê°’ì„ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì— í¬í•¨ í• ì§€ ì—¬ë¶€. Default to False.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        SGDRegressor
    """

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = get_hyper_params(classname=SGDRegressor)

    return __my_regression(
        classname=SGDRegressor,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        **params,
    )

@register_method
def my_rf_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = False,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    is_print: bool = True,
    **params,
) -> RandomForestRegressor:
    """RandomForest íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        SGDRegressor
    """

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = get_hyper_params(classname=RandomForestRegressor)

    return __my_regression(
        classname=RandomForestRegressor,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        **params,
    )

@register_method
def my_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = True,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    algorithm: list = None,
    scoring: list = ["rmse", "mse", "r2", "mae", "mape", "mpe", "rf"],
    **params,
) -> any:
    """íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        algorithm (list, optional): ì‚¬ìš©í•  ì•Œê³ ë¦¬ì¦˜ ["linear", "ridge", "lasso", "knn", "dtree", "svr", "sgd"]. Defaults to None.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        any
    """

    results = []  # ê²°ê³¼ê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    processes = []  # ë³‘ë ¬ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡œì„¸ìŠ¤ ë¦¬ìŠ¤íŠ¸
    estimators = {}  # íšŒê·€ë¶„ì„ ëª¨ë¸ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    estimator_names = []  # íšŒê·€ë¶„ì„ ëª¨ë¸ì˜ ì´ë¦„ì„ ì €ì¥í•  ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    callstack = []
    result_scores = []

    if not algorithm:
        # algorithm = ["linear", "ridge", "lasso", "knn", "dtree", "svr", "sgd", "rf"]
        algorithm = ["linear", "ridge", "lasso", "knn", "dtree", "svr", "sgd"]

    if "linear" in algorithm:
        callstack.append(my_linear_regression)

    if "ridge" in algorithm:
        callstack.append(my_ridge_regression)

    if "lasso" in algorithm:
        callstack.append(my_lasso_regression)

    if "knn" in algorithm:
        callstack.append(my_knn_regression)

    if "dtree" in algorithm:
        callstack.append(my_dtree_regression)

    if "svr" in algorithm:
        callstack.append(my_svr_regression)

    if "sgd" in algorithm:
        callstack.append(my_sgd_regression)

    if "rf" in algorithm:
        callstack.append(my_rf_regression)

    score_fields = []
    score_method = []

    for s in scoring:
        if s == "r2":
            score_fields.append("ê²°ì •ê³„ìˆ˜(R2)")
            score_method.append(True)
        elif s == "rmse":
            score_fields.append("í‰ê· ì˜¤ì°¨(RMSE)")
            score_method.append(False)
        elif s == "mae":
            score_fields.append("í‰ê· ì ˆëŒ€ì˜¤ì°¨(MAE)")
            score_method.append(False)
        elif s == "mse":
            score_fields.append("í‰ê· ì œê³±ì˜¤ì°¨(MSE)")
            score_method.append(False)
        elif s == "mape":
            score_fields.append("í‰ê·  ì ˆëŒ€ ë°±ë¶„ì˜¤ì°¨ ë¹„ìœ¨(MAPE)")
            score_method.append(False)
        elif s == "mpe":
            score_fields.append("í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨(MPE)")
            score_method.append(False)

    # ë³‘ë ¬ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡œì„¸ìŠ¤ ìƒì„± -> íšŒê·€ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ê°ê° í˜¸ì¶œí•œë‹¤.
    with futures.ThreadPoolExecutor() as executor:
        for c in callstack:
            if params:
                p = params.copy()

                if c != my_dtree_regression:
                    del p["pruning"]

            else:
                p = {}

            processes.append(
                executor.submit(
                    c,
                    x_train=x_train,
                    y_train=y_train,
                    x_test=x_test,
                    y_test=y_test,
                    cv=cv,
                    learning_curve=False,
                    report=False,
                    plot=False,
                    deg=1,
                    resid_test=False,
                    figsize=figsize,
                    dpi=dpi,
                    sort=False,
                    is_print=False,
                    **p,
                )
            )

        # ë³‘ë ¬ì²˜ë¦¬ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦°ë‹¤.
        for p in futures.as_completed(processes):
            # ê° íšŒê·€ í•¨ìˆ˜ì˜ ê²°ê³¼ê°’(íšŒê·€ëª¨í˜• ê°ì²´)ì„ ì €ì¥í•œë‹¤.
            estimator = p.result()

            if estimator is None:
                continue

            # íšŒê·€ëª¨í˜• ê°ì²´ê°€ í¬í•¨í•˜ê³  ìˆëŠ” ì„±ëŠ¥ í‰ê°€ì§€í‘œ(ë”•ì…”ë„ˆë¦¬)ë¥¼ ë³µì‚¬í•œë‹¤.
            scores = estimator.scores
            # íšŒê·€ëª¨í˜•ì˜ ì´ë¦„ê³¼ ê°ì²´ë¥¼ ì €ì¥í•œë‹¤.
            n = estimator.__class__.__name__
            estimator_names.append(n)
            estimators[n] = estimator
            # ì„±ëŠ¥í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
            results.append(scores)

            result_scores.append(
                {
                    "model": n,
                    "train": estimator.train_score,
                    "test": estimator.test_score,
                }
            )

        # ê²°ê³¼ê°’ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        print("\n\n==================== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ====================")
        result_df = DataFrame(results, index=estimator_names)

        if score_fields:
            result_df.sort_values(score_fields, ascending=score_method, inplace=True)

        my_pretty_table(result_df)

        score_df = DataFrame(data=result_scores, index=estimator_names).sort_values(
            by="test", ascending=False
        )
        score_df = score_df.melt(id_vars="model", var_name="data", value_name="score")
        my_barplot(
            df=score_df,
            yname="model",
            xname="score",
            hue="data",
            figsize=figsize,
            dpi=dpi,
            callback=lambda ax: ax.set_title("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"),
        )

    # ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ì„ íƒ
    if score_fields[0] == "ê²°ì •ê³„ìˆ˜(R2)":
        best_idx = result_df[score_fields[0]].idxmax()
    else:
        best_idx = result_df[score_fields[0]].idxmin()

    estimators["best"] = estimators[best_idx]

    my_regression_result(
        estimator=estimators["best"],
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        learning_curve=learning_curve,
        cv=cv,
        figsize=figsize,
        dpi=dpi,
        is_print=True,
    )

    if report:
        my_regression_report(
            estimator=estimators["best"],
            x_train=x_train,
            y_train=y_train,
            x_test=x_test,
            y_test=y_test,
            sort=sort,
            plot=plot,
            deg=deg,
            figsize=figsize,
            dpi=dpi,
        )

    if resid_test:
        my_resid_test(
            x=x_train,
            y=y_train,
            y_pred=estimators["best"].predict(x_train),
            figsize=figsize,
            dpi=dpi,
        )

    return estimators

@register_method
def my_voting_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    lr: bool = True,
    rg: bool = False,
    ls: bool = False,
    knn: bool = True,
    dtree: bool = False,
    svr: bool = False,
    sgd: bool = False,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = True,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
) -> VotingRegressor:
    """Voting ë¶„ë¥˜ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): í›ˆë ¨ ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜
        y_train (Series): í›ˆë ¨ ë°ì´í„°ì˜ ì¢…ì†ë³€ìˆ˜
        x_test (DataFrame, optional): ê²€ì¦ ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜. Defaults to None.
        y_test (Series, optional): ê²€ì¦ ë°ì´í„°ì˜ ì¢…ì†ë³€ìˆ˜. Defaults to None.
        lr (bool, optional): ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì„ ì‚¬ìš©í• ì§€ ì—¬ë¶€. Defaults to True.
        rg (bool, optional): ë¦¿ì§€ íšŒê·€ë¶„ì„ì„ ì‚¬ìš©í• ì§€ ì—¬ë¶€. Defaults to False.
        ls (bool, optional): ë¼ì˜ íšŒê·€ë¶„ì„ì„ ì‚¬ìš©í• ì§€ ì—¬ë¶€. Defaults to False.
        knn (bool, optional): KNN íšŒê·€ë¶„ì„ì„ ì‚¬ìš©í• ì§€ ì—¬ë¶€. Defaults to True.
        dtree (bool, optional): ì˜ì‚¬ê²°ì •ë‚˜ë¬´ íšŒê·€ë¶„ì„ì„ ì‚¬ìš©í• ì§€ ì—¬ë¶€. Defaults to False.
        svr (bool, optional): ì„œí¬íŠ¸ë²¡í„° íšŒê·€ë¶„ì„ì„ ì‚¬ìš©í• ì§€ ì—¬ë¶€. Defaults to False.
        sgd (bool, optional): SGD íšŒê·€ë¶„ì„ì„ ì‚¬ìš©í• ì§€ ì—¬ë¶€. Defaults to False.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        report (bool, optional): ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (str, optional): ì •ë ¬ ê¸°ì¤€. Defaults to None.
    """

    params = {}
    estimators = []

    if lr:
        estimators.append(("lr", get_estimator(classname=LinearRegression)))
        params.update(get_hyper_params(classname=LinearRegression, key="lr"))

    if rg:
        estimators.append(("rg", get_estimator(classname=Ridge)))
        params.update(get_hyper_params(classname=Ridge, key="rg"))

    if ls:
        estimators.append(("ls", get_estimator(classname=Lasso)))
        params.update(get_hyper_params(classname=Lasso, key="ls"))

    if knn:
        estimators.append(("knn", get_estimator(classname=KNeighborsRegressor)))
        params.update(get_hyper_params(classname=KNeighborsRegressor, key="knn"))

    if dtree:
        estimators.append(("dtree", get_estimator(classname=DecisionTreeRegressor)))
        params.update(get_hyper_params(classname=DecisionTreeRegressor, key="dtree"))

    if svr:
        estimators.append(("svr", get_estimator(classname=SVR)))
        params.update(get_hyper_params(classname=SVR, key="svr"))

    if sgd:
        estimators.append(("sgd", get_estimator(classname=SGDRegressor)))
        params.update(get_hyper_params(classname=SGDRegressor, key="sgd"))

    return __my_regression(
        classname=VotingRegressor,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=True,
        estimators=estimators,
        **params,
    )

@register_method
def my_bagging_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    estimator: type = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = True,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    algorithm: list = None,
    scoring: list = ["rmse", "mse", "r2", "mae", "mape", "mpe"],
    **params,
) -> DataFrame:
    """ë°°ê¹… ì•™ìƒë¸” ë¶„ë¥˜ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        estimator (type): ê¸°ë³¸ ë¶„ë¥˜ë¶„ì„ ì•Œê³ ë¦¬ì¦˜
        x_train (DataFrame): í›ˆë ¨ ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜
        y_train (Series): í›ˆë ¨ ë°ì´í„°ì˜ ì¢…ì†ë³€ìˆ˜
        x_test (DataFrame, optional): ê²€ì¦ ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜. Defaults to None.
        y_test (Series, optional): ê²€ì¦ ë°ì´í„°ì˜ ì¢…ì†ë³€ìˆ˜. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        algorithm: list = None,
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        DataFrame: ë¶„ë¥˜ë¶„ì„ ê²°ê³¼
    """

    if estimator is None:
        estimator = my_regression(
            x_train=x_train,
            y_train=y_train,
            x_test=x_test,
            y_test=y_test,
            cv=cv,
            learning_curve=learning_curve,
            report=False,
            plot=False,
            deg=deg,
            resid_test=False,
            figsize=figsize,
            dpi=dpi,
            sort=sort,
            algorithm=algorithm,
            scoring=scoring,
            **params,
        )

        estimator = estimator["best"]

    if type(estimator) is type:
        params = get_hyper_params(classname=estimator, key="estimator")
        estimator = get_estimator(classname=estimator)
    else:
        params = get_hyper_params(classname=estimator.__class__, key="estimator")

    bagging_params = get_hyper_params(classname=BaggingRegressor)
    params.update(bagging_params)

    return __my_regression(
        classname=BaggingRegressor,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=True,
        base_estimator=estimator,
        **params,
    )

@register_method
def my_ada_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    estimator: type = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = True,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    algorithm: list = None,
    scoring: list = ["rmse", "mse", "r2", "mae", "mape", "mpe"],
    **params,
) -> AdaBoostRegressor:
    """AdaBoost ì•™ìƒë¸” íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        estimator (type): ê¸°ë³¸ íšŒê·€ë¶„ì„ ì•Œê³ ë¦¬ì¦˜
        x_train (DataFrame): í›ˆë ¨ ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜
        y_train (Series): í›ˆë ¨ ë°ì´í„°ì˜ ì¢…ì†ë³€ìˆ˜
        x_test (DataFrame, optional): ê²€ì¦ ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜. Defaults to None.
        y_test (Series, optional): ê²€ì¦ ë°ì´í„°ì˜ ì¢…ì†ë³€ìˆ˜. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        algorithm: list = None,
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        AdaBoostRegressor
    """

    if estimator is None:
        estimator = my_regression(
            x_train=x_train,
            y_train=y_train,
            x_test=x_test,
            y_test=y_test,
            cv=cv,
            learning_curve=learning_curve,
            report=False,
            plot=False,
            deg=deg,
            resid_test=False,
            figsize=figsize,
            dpi=dpi,
            sort=sort,
            algorithm=algorithm,
            scoring=scoring,
            **params,
        )

        estimator = estimator["best"]

    if type(estimator) is type:
        params = get_hyper_params(classname=estimator, key="estimator")
        estimator = get_estimator(classname=estimator)
    else:
        params = get_hyper_params(classname=estimator.__class__, key="estimator")

    bagging_params = get_hyper_params(classname=AdaBoostRegressor)
    params.update(bagging_params)

    return __my_regression(
        classname=AdaBoostRegressor,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=True,
        base_estimator=estimator,
        **params,
    )

@register_method
def my_gbm_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = True,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    scoring: list = ["rmse", "mse", "r2", "mae", "mape", "mpe"],
    **params,
) -> GradientBoostingRegressor:
    """GradientBoosting ì•™ìƒë¸” íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): í›ˆë ¨ ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜
        y_train (Series): í›ˆë ¨ ë°ì´í„°ì˜ ì¢…ì†ë³€ìˆ˜
        x_test (DataFrame, optional): ê²€ì¦ ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜. Defaults to None.
        y_test (Series, optional): ê²€ì¦ ë°ì´í„°ì˜ ì¢…ì†ë³€ìˆ˜. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        algorithm: list = None,
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        GradientBoostingRegressor
    """

    params = get_hyper_params(classname=AdaBoostRegressor)

    return __my_regression(
        classname=GradientBoostingRegressor,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=True,
        **params,
    )


@register_method
def my_xgb_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    pruning: bool = False,
    learning_curve: bool = True,
    report=True,
    plot: bool = False,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    is_print: bool = True,
    **params,
) -> XGBRegressor:
    """XGBRegressor íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
        pruning (bool, optional): ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì—ì„œ ê°€ì§€ì¹˜ê¸°ì˜ alphaê°’ì„ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì— í¬í•¨ í• ì§€ ì—¬ë¶€. Default to False.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        XGBRegressor
    """

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = get_hyper_params(classname=XGBRegressor)

    return __my_regression(
        classname=XGBRegressor,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        **params,
    )

@register_method
def my_lgbm_regression(
    x_train: DataFrame,
    y_train: Series,
    x_test: DataFrame = None,
    y_test: Series = None,
    cv: int = 5,
    learning_curve: bool = True,
    report=True,
    plot: bool = False,
    deg: int = 1,
    resid_test=False,
    figsize=(10, 5),
    dpi: int = 100,
    sort: str = None,
    is_print: bool = True,
    **params,
) -> LGBMRegressor:
    """LGBMRegressor íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        deg (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        is_print (bool, optional): ì¶œë ¥ ì—¬ë¶€. Defaults to True.
        pruning (bool, optional): ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì—ì„œ ê°€ì§€ì¹˜ê¸°ì˜ alphaê°’ì„ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì— í¬í•¨ í• ì§€ ì—¬ë¶€. Default to False.
        **params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to None.

    Returns:
        LGBMRegressor
    """

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        if not params:
            params = get_hyper_params(classname=LGBMRegressor)

    return __my_regression(
        classname=LGBMRegressor,
        x_train=x_train,
        y_train=y_train,
        x_test=x_test,
        y_test=y_test,
        cv=cv,
        learning_curve=learning_curve,
        report=report,
        plot=plot,
        deg=deg,
        resid_test=resid_test,
        figsize=figsize,
        dpi=dpi,
        sort=sort,
        is_print=is_print,
        **params,
    )
